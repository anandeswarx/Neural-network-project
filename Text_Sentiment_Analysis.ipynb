{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc04a51",
   "metadata": {},
   "source": [
    "<h1><right>Dataset</center></h1>\n",
    " \n",
    "#### **Dataset Download Reference Link : http://ai.stanford.edu/~amaas/data/sentiment/**\n",
    "#### > Large Movie Review Dataset \n",
    "#### > This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. \n",
    "#### > This Dataset contains 25,000 highly polar movie reviews for training, and 25,000 for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0f2bf",
   "metadata": {},
   "source": [
    "#### Dowload the Dataset & Install other helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700df5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O aclImdb_v1.tar.gz\n",
    "# !tar -xvzf aclImdb_v1.tar.gz\n",
    "# !wget http://nlp.uoregon.edu/download/embeddings/glove.6B.50d.txt -O glove.6B.50d.txt\n",
    "\n",
    "# !python3 -m pip install --upgrade pip\n",
    "# !pip3 install nltk\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade53a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 10:06:15.974136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d34724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH_POS = \"aclImdb/train/pos/\"\n",
    "TRAIN_PATH_NEG = \"aclImdb/train/neg/\"\n",
    "\n",
    "TEST_PATH_POS = \"aclImdb/test/pos/\"\n",
    "TEST_PATH_NEG = \"aclImdb/test/neg/\"\n",
    "\n",
    "GLOVE_PATH = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c6fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "min_length = 30\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8113e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadDataSet:\n",
    "\n",
    "    def __init__(self, path, label, min_length=30, max_length=200):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.dataset = []\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        \n",
    "    def parse(self):\n",
    "        self.label_array = [1,0] if self.label == \"positive\" else [0,1]\n",
    "        for root, subfolder, file_names in os.walk(self.path):\n",
    "            for file_name in file_names:\n",
    "                temp_file = open(os.path.join(root, file_name), encoding=\"utf8\")\n",
    "                text = temp_file.readline()\n",
    "                length = sum([1 for word in text.split(\" \") if word])\n",
    "                if length > self.min_length and length < self.max_length+1:\n",
    "                    self.dataset.extend([{\"FileName\":file_name, \"Label\":self.label_array, \"Text\":text}])\n",
    "                temp_file.close()\n",
    "        return pd.DataFrame(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523dd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_samples = ReadDataSet(TRAIN_PATH_POS, \"positive\").parse()\n",
    "train_neg_samples = ReadDataSet(TRAIN_PATH_NEG, \"negative\").parse()\n",
    "\n",
    "test_pos_samples = ReadDataSet(TEST_PATH_POS, \"positive\").parse()\n",
    "test_neg_samples = ReadDataSet(TEST_PATH_NEG, \"negative\").parse()\n",
    "\n",
    "train_samples = pd.concat([train_pos_samples, train_neg_samples], ignore_index=True)\n",
    "test_samples  = pd.concat([test_pos_samples, test_neg_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e5991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanDataSet:\n",
    "    \n",
    "    def __init__(self,data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        \n",
    "    def remove_punctuations(self,text):\n",
    "        text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        return text_nopunct  \n",
    "\n",
    "    def remove_extraspaces(self, text):\n",
    "        return [ item for item in text.split() if item]\n",
    "    \n",
    "    def clean(self):\n",
    "        self.data_frame[\"Text\"]   = self.data_frame[\"Text\"].apply(lambda x: x.lower())\n",
    "        self.data_frame[\"Text\"]   = self.data_frame[\"Text\"].apply(lambda x: self.remove_punctuations(x))\n",
    "        self.data_frame[\"Text\"]   = self.data_frame[\"Text\"].apply(lambda x: self.remove_extraspaces(x))\n",
    "        self.data_frame[\"Length\"] = self.data_frame[\"Text\"].apply(lambda x: len(x))\n",
    "        return self.data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2930690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = CleanDataSet(train_samples).clean()\n",
    "test_samples = CleanDataSet(test_samples).clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3094f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \n",
    "    def __init__(self, glove_path):\n",
    "        \n",
    "        self.glove_path = glove_path\n",
    "        self.glove_dict = {}\n",
    "        self.read_glove()\n",
    "        self.ps = nltk.PorterStemmer()\n",
    "\n",
    "    def read_glove(self):\n",
    "        glove_file = open(self.glove_path, encoding=\"utf8\")\n",
    "        for line in glove_file.readlines():\n",
    "            line_list = line.split(\" \")\n",
    "            temp_list = line_list[1:-1]\n",
    "            temp_list.append(line_list[-1].split(\"\\n\")[0])\n",
    "            float_vector = [float(vector) for vector in temp_list]\n",
    "            self.glove_dict[line_list[0]] = np.array(float_vector)\n",
    "        glove_file.close()\n",
    "        \n",
    "    def glove_formatting(self, x):\n",
    "        vec_list = []\n",
    "        for item in x:\n",
    "            try:\n",
    "                vec_list.append(self.glove_dict[item])\n",
    "            except:\n",
    "                try:\n",
    "                    vec_list.append(self.glove_dict[self.ps.stem(item)])\n",
    "                except:\n",
    "                    vec_list.append(np.array([0.0 for i in range(50)]))\n",
    "        return np.array(vec_list).astype(\"float16\")\n",
    "    \n",
    "    def encode(self, data_frame):\n",
    "        data_frame[\"Embedding\"] = data_frame[\"Text\"].apply(lambda x : self.glove_formatting(x))\n",
    "        return data_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab50bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(GLOVE_PATH)\n",
    "train_samples = encoder.encode(train_samples)\n",
    "test_samples = encoder.encode(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ff16c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Length</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>9599_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[boring, badly, written, italian, exploitation...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[-0.03568, -0.4177, -0.441, -0.3545, -0.2876,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>3935_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[cool, idea, botched, writing, botched, direct...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[-0.656, 0.4565, -0.1675, -0.5835, -0.2307, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>2188_10.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[with, very, little, screen, time, and, money,...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[0.256, 0.437, -0.1189, 0.2035, 0.4197, 0.858...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>2267_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[please, avoid, this, movie, at, all, costs, t...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[-0.434, 0.7397, 0.784, -0.4192, 0.479, -0.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>10387_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[giant, crabs, cursing, in, japanese, what, wa...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[1.247, -0.2174, 0.4365, 1.927, 0.499, 0.2177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8942</th>\n",
       "      <td>1750_4.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[priyadarshans, hera, pheri, was, a, nice, sit...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.1812, -1.669, -0.3264, -0.1554, -1.024, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>12494_8.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[finally, an, indie, film, that, actually, del...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.1626, -0.3728, -0.02248, -0.505, 0.2747, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>3246_9.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[i, first, saw, this, movie, on, cable, about,...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>6229_7.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[one, of, several, musicals, about, sailors, o...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.3147, 0.4165, 0.1348, 0.1586, 0.888, 0.433...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>8417_8.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[while, this, movies, style, isnt, as, underst...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.1011, -0.1656, 0.2203, -0.10626, 0.4692, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FileName   Label                                               Text  \\\n",
       "11541   9599_1.txt  [0, 1]  [boring, badly, written, italian, exploitation...   \n",
       "8655    3935_1.txt  [0, 1]  [cool, idea, botched, writing, botched, direct...   \n",
       "859    2188_10.txt  [1, 0]  [with, very, little, screen, time, and, money,...   \n",
       "7550    2267_1.txt  [0, 1]  [please, avoid, this, movie, at, all, costs, t...   \n",
       "10861  10387_1.txt  [0, 1]  [giant, crabs, cursing, in, japanese, what, wa...   \n",
       "...            ...     ...                                                ...   \n",
       "8942    1750_4.txt  [0, 1]  [priyadarshans, hera, pheri, was, a, nice, sit...   \n",
       "721    12494_8.txt  [1, 0]  [finally, an, indie, film, that, actually, del...   \n",
       "1917    3246_9.txt  [1, 0]  [i, first, saw, this, movie, on, cable, about,...   \n",
       "1989    6229_7.txt  [1, 0]  [one, of, several, musicals, about, sailors, o...   \n",
       "6832    8417_8.txt  [1, 0]  [while, this, movies, style, isnt, as, underst...   \n",
       "\n",
       "       Length                                          Embedding  \n",
       "11541      31  [[-0.03568, -0.4177, -0.441, -0.3545, -0.2876,...  \n",
       "8655       31  [[-0.656, 0.4565, -0.1675, -0.5835, -0.2307, -...  \n",
       "859        31  [[0.256, 0.437, -0.1189, 0.2035, 0.4197, 0.858...  \n",
       "7550       31  [[-0.434, 0.7397, 0.784, -0.4192, 0.479, -0.90...  \n",
       "10861      31  [[1.247, -0.2174, 0.4365, 1.927, 0.499, 0.2177...  \n",
       "...       ...                                                ...  \n",
       "8942      200  [[0.1812, -1.669, -0.3264, -0.1554, -1.024, 0....  \n",
       "721       200  [[0.1626, -0.3728, -0.02248, -0.505, 0.2747, -...  \n",
       "1917      200  [[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....  \n",
       "1989      200  [[0.3147, 0.4165, 0.1348, 0.1586, 0.888, 0.433...  \n",
       "6832      200  [[0.1011, -0.1656, 0.2203, -0.10626, 0.4692, 0...  \n",
       "\n",
       "[14593 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.sort_values(by=\"Embedding\", key=lambda x: x.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54d3cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Length</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>2164_4.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[my, first, thoughts, on, this, film, were, of...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[-0.2727, 0.7754, -0.1018, -0.9165, 0.905, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>11360_10.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[i, smiled, through, the, whole, film, the, mu...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11777</th>\n",
       "      <td>7628_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[what, a, stinker, i, swear, this, movie, was,...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[0.4531, 0.0598, -0.1058, -0.333, 0.7236, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13673</th>\n",
       "      <td>361_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[i, have, never, fallen, asleep, whilst, watch...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>5255_1.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[im, sure, he, doesnt, need, the, money, for, ...</td>\n",
       "      <td>31</td>\n",
       "      <td>[[-0.0677, 0.5186, 1.326, -0.3867, -0.7974, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12102</th>\n",
       "      <td>6877_3.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[here, he, is, a, new, horror, icon, for, the,...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.141, 0.682, -0.504, 0.383, 0.6343, -1.186,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13108</th>\n",
       "      <td>9483_2.txt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[the, boys, were, the, most, appealing, things...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.418, 0.2496, -0.4124, 0.1217, 0.3452, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>6346_10.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[ive, seen, lonesome, dove, dead, mans, walk, ...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.2286, -0.7954, -0.4978, -1.086, 0.03766, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>10321_10.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[this, is, the, very, first, three, stooges, s...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.531, 0.4011, -0.408, 0.1544, 0.4778, 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>7336_7.txt</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[this, one, what, came, to, my, mind, immediat...</td>\n",
       "      <td>200</td>\n",
       "      <td>[[0.531, 0.4011, -0.408, 0.1544, 0.4778, 0.207...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14855 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           FileName   Label  \\\n",
       "12521    2164_4.txt  [0, 1]   \n",
       "7168   11360_10.txt  [1, 0]   \n",
       "11777    7628_1.txt  [0, 1]   \n",
       "13673     361_1.txt  [0, 1]   \n",
       "10430    5255_1.txt  [0, 1]   \n",
       "...             ...     ...   \n",
       "12102    6877_3.txt  [0, 1]   \n",
       "13108    9483_2.txt  [0, 1]   \n",
       "4599    6346_10.txt  [1, 0]   \n",
       "1339   10321_10.txt  [1, 0]   \n",
       "5918     7336_7.txt  [1, 0]   \n",
       "\n",
       "                                                    Text  Length  \\\n",
       "12521  [my, first, thoughts, on, this, film, were, of...      31   \n",
       "7168   [i, smiled, through, the, whole, film, the, mu...      31   \n",
       "11777  [what, a, stinker, i, swear, this, movie, was,...      31   \n",
       "13673  [i, have, never, fallen, asleep, whilst, watch...      31   \n",
       "10430  [im, sure, he, doesnt, need, the, money, for, ...      31   \n",
       "...                                                  ...     ...   \n",
       "12102  [here, he, is, a, new, horror, icon, for, the,...     200   \n",
       "13108  [the, boys, were, the, most, appealing, things...     200   \n",
       "4599   [ive, seen, lonesome, dove, dead, mans, walk, ...     200   \n",
       "1339   [this, is, the, very, first, three, stooges, s...     200   \n",
       "5918   [this, one, what, came, to, my, mind, immediat...     200   \n",
       "\n",
       "                                               Embedding  \n",
       "12521  [[-0.2727, 0.7754, -0.1018, -0.9165, 0.905, -0...  \n",
       "7168   [[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....  \n",
       "11777  [[0.4531, 0.0598, -0.1058, -0.333, 0.7236, -0....  \n",
       "13673  [[0.1189, 0.1526, -0.0821, -0.741, 0.7593, -0....  \n",
       "10430  [[-0.0677, 0.5186, 1.326, -0.3867, -0.7974, -1...  \n",
       "...                                                  ...  \n",
       "12102  [[0.141, 0.682, -0.504, 0.383, 0.6343, -1.186,...  \n",
       "13108  [[0.418, 0.2496, -0.4124, 0.1217, 0.3452, -0.0...  \n",
       "4599   [[0.2286, -0.7954, -0.4978, -1.086, 0.03766, -...  \n",
       "1339   [[0.531, 0.4011, -0.408, 0.1544, 0.4778, 0.207...  \n",
       "5918   [[0.531, 0.4011, -0.408, 0.1544, 0.4778, 0.207...  \n",
       "\n",
       "[14855 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples.sort_values(by=\"Embedding\", key=lambda x: x.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c94f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, CellDim=50, Classes=2, NumLayers=1, DropRate=0.4):\n",
    "        \n",
    "        self.input = tf.placeholder(shape=(None, None, CellDim), dtype=\"float32\", name=\"input\")\n",
    "        self.input_length = tf.placeholder(shape=(None), dtype=\"int32\", name=\"input_length\")\n",
    "        self.drop_rate = tf.placeholder(dtype=tf.float32, name=\"dropout_rate\")\n",
    "        self.ground_truth = tf.placeholder(shape=(None, Classes), dtype=\"float32\", name=\"groundtruth\")\n",
    "        self.CellDim = CellDim\n",
    "        self.Classes = Classes\n",
    "        self.NumLayers = NumLayers\n",
    "        self.DropRate = DropRate\n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        output, state = tf.nn.dynamic_rnn(cell=tf.nn.rnn_cell.LSTMCell(self.CellDim), \n",
    "                                          inputs=self.input, \n",
    "                                          sequence_length=self.input_length, \n",
    "                                          dtype=\"float32\")\n",
    "        \n",
    "        for i in range(self.NumLayers - 1):\n",
    "            with tf.variable_scope(\"Layer_\"+str(i)):\n",
    "                output, state = tf.nn.dynamic_rnn(cell=tf.nn.rnn_cell.LSTMCell(self.CellDim), \n",
    "                                              inputs=output, \n",
    "                                              sequence_length=self.input_length, \n",
    "                                              dtype=\"float32\")\n",
    "            \n",
    "        dropout = tf.nn.dropout(state.h, rate=self.DropRate)\n",
    "        self.predictions = tf.layers.dense(dropout, units=self.Classes, activation=tf.nn.softmax, name=\"predictions\")\n",
    "        \n",
    "        return self.input, self.input_length, self.drop_rate, self.ground_truth, self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ada2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_Model = Model(CellDim=50, Classes=2, NumLayers=1, DropRate=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1910d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_3795/2357509329.py:16: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/layers/rnn/legacy_cells.py:1042: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3795/2357509329.py:16: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  output, state = tf.nn.dynamic_rnn(cell=tf.nn.rnn_cell.LSTMCell(self.CellDim),\n",
      "/tmp/ipykernel_3795/2357509329.py:29: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.predictions = tf.layers.dense(dropout, units=self.Classes, activation=tf.nn.softmax, name=\"predictions\")\n"
     ]
    }
   ],
   "source": [
    "input_tensor, input_length, drop_rate, ground_truth, predictions = SA_Model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ca4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, ground_truth, predictions):\n",
    "        self.ground_truth = ground_truth\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def metrics(self):                                                                                                            \n",
    "        self.loss = tf.compat.v1.losses.softmax_cross_entropy(self.ground_truth, self.predictions)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.predictions, 1), \n",
    "                                                   tf.argmax(self.ground_truth, 1)), \"float32\"))\n",
    "        return self.loss, self.accuracy\n",
    "    \n",
    "    def apply(self):\n",
    "        self.loss, self.accuracy = self.metrics()\n",
    "        Optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        Session = tf.Session()\n",
    "        Session.run(tf.global_variables_initializer())\n",
    "        return Optimizer, Session, self.loss, self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f94ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 10:06:29.888569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13604 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:81:00.0, compute capability: 7.5\n",
      "2023-08-25 10:06:29.894474: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "Optimizer, Session, loss, accuracy = Trainer(ground_truth, predictions).apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c480c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(Model):\n",
    "    \n",
    "    def __init__(self, train_data, test_data, input_tensor, input_length, drop_rate, ground_truth, predictions, \\\n",
    "                 Optimizer, Session, Loss, Accuracy, BATCH_SIZE, Epochs):\n",
    "        \n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.input_tensor = input_tensor\n",
    "        self.input_length = input_length\n",
    "        self.droprate= drop_rate\n",
    "        self.prediction = predictions\n",
    "        \n",
    "        self.Optimizer = Optimizer\n",
    "        self.Session = Session\n",
    "        self.Loss = Loss\n",
    "        self.Accuracy = Accuracy\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Epochs = Epochs\n",
    "        \n",
    "        self.run()\n",
    "                                                \n",
    "    def train(self, input_data):\n",
    "        \n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for j in range(len(input_data)//BATCH_SIZE):\n",
    "\n",
    "            input_batch = []\n",
    "            groundtruth_batch = []\n",
    "            length_batch = []\n",
    "\n",
    "            for i in range(j, j+BATCH_SIZE) :\n",
    "                groundtruth_batch.append(input_data[\"Label\"][i])\n",
    "                length_batch.append(input_data[\"Length\"][i])\n",
    "\n",
    "            \n",
    "            for i in range(j, j+BATCH_SIZE): \n",
    "                data_array = np.zeros([max(length_batch), 50]).astype(\"float32\")\n",
    "                (a,b) = input_data[\"Embedding\"][i].shape\n",
    "                data_array[:a, :b] = input_data[\"Embedding\"][i].astype(\"float32\")\n",
    "                input_batch.append(data_array)\n",
    "                \n",
    "            _, batch_loss, batch_accuracy = self.Session.run([Optimizer, loss, accuracy], \n",
    "                                                 feed_dict={input_tensor: np.array(input_batch).astype(\"float32\"),\n",
    "                                                            input_length : np.array(length_batch).astype(\"float32\"),\n",
    "                                                            drop_rate : 0.4, \n",
    "                                                            ground_truth : np.array(groundtruth_batch).astype(\"float32\")})\n",
    "            loss.append(batch_loss)\n",
    "            accuracy.append(batch_accuracy)\n",
    "        print(\"Train Loss : {:.5f}    |   Train Accuracy : {:.5f}\".format(sum(self.loss)/len(self.loss), sum(self.accuracy)/len(self.accuracy)))\n",
    "        \n",
    "    def test(self, input_data):\n",
    "        \n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for j in range(len(input_data)//BATCH_SIZE):\n",
    "\n",
    "            input_batch = []\n",
    "            groundtruth_batch = []\n",
    "            length_batch = []\n",
    "\n",
    "            for i in range(j, j+BATCH_SIZE) :\n",
    "                groundtruth_batch.append(input_data[\"Label\"][i])\n",
    "                length_batch.append(input_data[\"Length\"][i])\n",
    "\n",
    "            \n",
    "            for i in range(j, j+BATCH_SIZE): \n",
    "                data_array = np.zeros([max(length_batch), 50]).astype(\"float32\")\n",
    "                (a,b) = input_data[\"Embedding\"][i].shape\n",
    "                data_array[:a, :b] = input_data[\"Embedding\"][i].astype(\"float32\")\n",
    "                input_batch.append(data_array)\n",
    "                \n",
    "            batch_loss, batch_accuracy = self.Session.run([loss, accuracy], \n",
    "                                                 feed_dict={input_tensor: np.array(input_batch).astype(\"float32\"),\n",
    "                                                            input_length : np.array(length_batch).astype(\"float32\"),\n",
    "                                                            drop_rate : 0.0, \n",
    "                                                            ground_truth : np.array(groundtruth_batch).astype(\"float32\")})\n",
    "            \n",
    "            loss.append(batch_loss)\n",
    "            accuracy.append(batch_accuracy)\n",
    "        print(\"Test  Loss : {:.5f}    |   Test  Accuracy : {:.5f}\".format(sum(loss)/len(loss), sum(accuracy)/len(accuracy)))\n",
    "\n",
    "    def run(self):\n",
    "        print(\"*\"*100)\n",
    "        print(\"Initial Run on Data Set without Optimizer : \")\n",
    "        self.test(self.train_data)\n",
    "        self.test(self.test_data)\n",
    "        for n in range(self.Epochs):\n",
    "            print(\"*\"*100)\n",
    "            print(\"Epoch : {:03d}\".format(n+1))\n",
    "            self.train(self.train_data)\n",
    "            self.test(self.test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Initial Run on Trian Set without Optimizer : \n",
      "Test Loss  : 0.68182    |   Test Accuracy  : 0.57319\n",
      "Test Loss  : 0.68183    |   Test Accuracy  : 0.56102\n",
      "****************************************************************************************************\n",
      "Epoch : 001\n",
      "Train Loss : 0.36775    |   Train Accuracy : 0.98643\n",
      "Test Loss  : 0.31585    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 002\n",
      "Train Loss : 0.31452    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31396    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 003\n",
      "Train Loss : 0.31374    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31363    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 004\n",
      "Train Loss : 0.31353    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31349    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 005\n",
      "Train Loss : 0.31344    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31342    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 006\n",
      "Train Loss : 0.31339    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31337    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 007\n",
      "Train Loss : 0.31335    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31335    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 008\n",
      "Train Loss : 0.31333    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31333    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 009\n",
      "Train Loss : 0.31332    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31331    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 010\n",
      "Train Loss : 0.31330    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31330    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 011\n",
      "Train Loss : 0.31330    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31329    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 012\n",
      "Train Loss : 0.31329    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31329    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 013\n",
      "Train Loss : 0.31328    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31328    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 014\n",
      "Train Loss : 0.31328    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31328    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 015\n",
      "Train Loss : 0.31328    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31328    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 016\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 017\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 018\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 019\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 020\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 021\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31327    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 022\n",
      "Train Loss : 0.31327    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 023\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 024\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 025\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 026\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 027\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 028\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 029\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 030\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 031\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 032\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 033\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 034\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 035\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 036\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 038\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 039\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 040\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 041\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 042\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 043\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 044\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 045\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 046\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 047\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 048\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 049\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 050\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 051\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 052\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 053\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 054\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 055\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 056\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 057\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 058\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 059\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 060\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 061\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 062\n",
      "Train Loss : 0.31326    |   Train Accuracy : 1.00000\n",
      "Test Loss  : 0.31326    |   Test Accuracy  : 1.00000\n",
      "****************************************************************************************************\n",
      "Epoch : 063\n"
     ]
    }
   ],
   "source": [
    "RUNNER = Train(train_samples, test_samples, input_tensor, input_length, drop_rate, ground_truth, predictions, \\\n",
    "                 Optimizer, Session, loss, accuracy, BATCH_SIZE, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941709b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a055be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2c695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
